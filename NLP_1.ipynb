{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkMHFBkL-Jun",
        "outputId": "55fea23a-f597-4afa-db14-3ecf25b4c0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Wrote a paragraph about AGI and save this as agi.text file.\n",
        "from google.colab import files\n",
        "upld=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "wDfZn33ZAeS_",
        "outputId": "2c0dd48f-68be-4ff8-efe6-f671bde10478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-16849678-236e-4155-a706-8fec8accd564\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-16849678-236e-4155-a706-8fec8accd564\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving agi.txt to agi (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Read and displayed this file with Python program.\n",
        "f=open('agi.txt', 'r', encoding='utf8', errors='ignore')\n",
        "content=f.read()\n"
      ],
      "metadata": {
        "id": "yMtOOleHBh1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Extracted sentences and words from this file.\n",
        "totw=[]\n",
        "sentnc=nltk.sent_tokenize(content)\n",
        "for echsntc in sentnc:\n",
        "  totw=totw+nltk.word_tokenize(echsntc)\n",
        "print(\"Sentences: \",sentnc)\n",
        "print(\"words: \",totw)\n",
        "print(\"Total no. of sentences\",len(sentnc))\n",
        "print(len(set(totw)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jCSG-AhHYIB",
        "outputId": "65bee3a8-61b0-406e-db1c-08dd014425a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:  ['Artificial general intelligence (AGI) is a field of theoretical AI research that attempts tot create software with human-like intelligence and the ability tot self-teach.', 'The aim is for the software tot be able tot perform tasks that it is not necessarily trained or developed for.', 'Current artificial intelligence (AI) technologies all function within a set of pre-determined parameters.', 'For example, AI models trained in image recognition and generation cannot build websites.', 'AGI is a theoretical pursuit tot develop AI systems that possess autonomous self-control, a reasonable degree of self-understanding, and the ability tot learn new skills.', 'It can solve complex problems in settings and contexts that were not taught tot it at the time of its creation.', 'AGI with human abilities remains a theoretical concept and research goal.']\n",
            "words:  ['Artificial', 'general', 'intelligence', '(', 'AGI', ')', 'is', 'a', 'field', 'of', 'theoretical', 'AI', 'research', 'that', 'attempts', 'tot', 'create', 'software', 'with', 'human-like', 'intelligence', 'and', 'the', 'ability', 'tot', 'self-teach', '.', 'The', 'aim', 'is', 'for', 'the', 'software', 'tot', 'be', 'able', 'tot', 'perform', 'tasks', 'that', 'it', 'is', 'not', 'necessarily', 'trained', 'or', 'developed', 'for', '.', 'Current', 'artificial', 'intelligence', '(', 'AI', ')', 'technologies', 'all', 'function', 'within', 'a', 'set', 'of', 'pre-determined', 'parameters', '.', 'For', 'example', ',', 'AI', 'models', 'trained', 'in', 'image', 'recognition', 'and', 'generation', 'can', 'not', 'build', 'websites', '.', 'AGI', 'is', 'a', 'theoretical', 'pursuit', 'tot', 'develop', 'AI', 'systems', 'that', 'possess', 'autonomous', 'self-control', ',', 'a', 'reasonable', 'degree', 'of', 'self-understanding', ',', 'and', 'the', 'ability', 'tot', 'learn', 'new', 'skills', '.', 'It', 'can', 'solve', 'complex', 'problems', 'in', 'settings', 'and', 'contexts', 'that', 'were', 'not', 'taught', 'tot', 'it', 'at', 'the', 'time', 'of', 'its', 'creation', '.', 'AGI', 'with', 'human', 'abilities', 'remains', 'a', 'theoretical', 'concept', 'and', 'research', 'goal', '.']\n",
            "Total no. of sentences 7\n",
            "87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing package to remove stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stpw=stopwords.words('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgP1_j7pDAKp",
        "outputId": "075fd6a5-0b87-4d3c-e90a-cbd1fc43c58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. There are any stop-words, removed them.\n",
        "stpwrd=[]\n",
        "nonstpwrd=[]\n",
        "for wrd in totw:\n",
        "  if wrd in stpw:\n",
        "    stpwrd.append(wrd)\n",
        "  else:\n",
        "    nonstpwrd.append(wrd)\n",
        "print(\"stopword list\", stpwrd)\n",
        "print(\"Non stop words list\", nonstpwrd)\n",
        "\n",
        "#5. Count number of stop words and non stop-words\n",
        "print(\"Length of stopwords:\", len(stpwrd))\n",
        "print(\"Length of non stop words\", len(nonstpwrd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aIzHzYAE8oE",
        "outputId": "6d4ccb20-9ae7-4574-caab-47190e0d8195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopword list ['is', 'a', 'of', 'that', 'with', 'and', 'the', 'is', 'for', 'the', 'be', 'that', 'it', 'is', 'not', 'or', 'for', 'all', 'a', 'of', 'in', 'and', 'can', 'not', 'is', 'a', 'that', 'a', 'of', 'and', 'the', 'can', 'in', 'and', 'that', 'were', 'not', 'it', 'at', 'the', 'of', 'its', 'with', 'a', 'and']\n",
            "Non stop words list ['Artificial', 'general', 'intelligence', '(', 'AGI', ')', 'field', 'theoretical', 'AI', 'research', 'attempts', 'tot', 'create', 'software', 'human-like', 'intelligence', 'ability', 'tot', 'self-teach', '.', 'The', 'aim', 'software', 'tot', 'able', 'tot', 'perform', 'tasks', 'necessarily', 'trained', 'developed', '.', 'Current', 'artificial', 'intelligence', '(', 'AI', ')', 'technologies', 'function', 'within', 'set', 'pre-determined', 'parameters', '.', 'For', 'example', ',', 'AI', 'models', 'trained', 'image', 'recognition', 'generation', 'build', 'websites', '.', 'AGI', 'theoretical', 'pursuit', 'tot', 'develop', 'AI', 'systems', 'possess', 'autonomous', 'self-control', ',', 'reasonable', 'degree', 'self-understanding', ',', 'ability', 'tot', 'learn', 'new', 'skills', '.', 'It', 'solve', 'complex', 'problems', 'settings', 'contexts', 'taught', 'tot', 'time', 'creation', '.', 'AGI', 'human', 'abilities', 'remains', 'theoretical', 'concept', 'research', 'goal', '.']\n",
            "Length of stopwords: 45\n",
            "Length of non stop words 98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist=FreqDist(nonstpwrd)\n",
        "print(fdist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g17bi4pGaiB",
        "outputId": "69254026-6e5c-40e2-dfc1-5e538c4ed8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 69 samples and 98 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.tabulate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFBjUNJxGxnG",
        "outputId": "ba3688c9-eaae-474e-991c-0226cad61fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               tot                  .                 AI       intelligence                AGI        theoretical                  ,                  (                  )           research           software            ability            trained         Artificial            general              field           attempts             create         human-like         self-teach                The                aim               able            perform              tasks        necessarily          developed            Current         artificial       technologies           function             within                set     pre-determined         parameters                For            example             models              image        recognition         generation              build           websites            pursuit            develop            systems            possess         autonomous       self-control         reasonable             degree self-understanding              learn                new             skills                 It              solve            complex           problems           settings           contexts             taught               time           creation              human          abilities            remains            concept               goal \n",
            "                 7                  7                  4                  3                  3                  3                  3                  2                  2                  2                  2                  2                  2                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1                  1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XhZgylDBIKj_",
        "outputId": "82756a80-e0af-4c78-88df-1c80262e9666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tot'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fdist.min()"
      ],
      "metadata": {
        "id": "oA1wCd14IRnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('inlkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seKBiIIvIp1T",
        "outputId": "4297e121-639a-46ae-8d3a-5466ac209c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading inlkt: Package 'inlkt' not found in index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOPOHe6g55sx",
        "outputId": "03dccf7a-7d97-4aa2-8c60-3231dbc5fc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "\n",
        "# Extract text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "# Remove stop words and tokenize using nltk\n",
        "def remove_stopwords_and_tokenize(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return filtered_text\n",
        "\n",
        "# Extract meaningful words using spaCy\n",
        "def extract_meaningful_words(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    meaningful_words = [token.lemma_ for token in doc if not token.is_punct]\n",
        "    return meaningful_words\n",
        "\n",
        "# Create index like dictionary\n",
        "def create_index(text):\n",
        "    words = remove_stopwords_and_tokenize(text)\n",
        "    meaningful_words = extract_meaningful_words(' '.join(words))\n",
        "    index_dict = {}\n",
        "    for word in meaningful_words:\n",
        "        index_dict[word] = index_dict.get(word, 0) + 1\n",
        "    return index_dict\n",
        "\n",
        "pdf_text = extract_text_from_pdf('B.pdf')\n",
        "index = create_index(pdf_text)\n",
        "print(index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "nYmgCLWE3q6O",
        "outputId": "9705d755-3cbf-4e5b-ba3d-045a0fd2ec8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fitz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-32a520b36d70>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uy-QBcmB5ufI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inltk\n",
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html!"
      ],
      "metadata": {
        "id": "abe4ofMUrdcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import abc,  Counter, defaultdict, namedtuple, OrderedDict\n",
        "from collections.abc import Iterable"
      ],
      "metadata": {
        "id": "nW_2RKa0sZSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inltk.inltk import setup\n",
        "from collections.abc import Iterable\n",
        "setup('ta')"
      ],
      "metadata": {
        "id": "K6UUA057AX4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Extracted sentences and words from this file.\n",
        "content=\"விடிந்ததும் சிரிக்கிறேன் அதுவரை அழுகிறேன் ஆறாத துயரங்கள் அணுவினில் கலந்திருக்கையில் யாதும் அறியாமல் தவிக்கிறேன் வினையூக்கியாய் இவ்விரவது இருளினை ஊற்றுகையில் கண்ணீரின் நிறங்கள் மாறுவதை என் குறிப்பேட்டில் எழுதி கொள்கிறேன் நேற்றுவரை தென்றல் என்றவை இன்று முதல் பாதகமென்பதை உணர்ந்ததால் சொல்கிறேன் விடியும் வரை அழுகிறேன் விழியினில் நிறைந்து வெளிவருந் சிறு துளியின் பாரமது ஒவ்வோர்…\"\n",
        "\n",
        "totw=[]\n",
        "sentnc=inltk.sent_tokenize(content)\n",
        "for echsntc in sentnc:\n",
        "  totw=totw+inltk.word_tokenize(echsntc)\n",
        "print(\"Sentences: \",sentnc)\n",
        "print(\"words: \",totw)\n",
        "print(\"Total no. of sentences\",len(sentnc))\n",
        "print(len(set(totw)))"
      ],
      "metadata": {
        "id": "6jdeg4ri_fNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install translate\n",
        "from translate import Translator\n",
        "ln=Translator(to_lang=\"ta\")\n",
        "for i in range(1,10):\n",
        "  tamil_txt=ln.translate(nonstpwrd[i])\n",
        "  print(tamil_txt)\n"
      ],
      "metadata": {
        "id": "T5VymY67stRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. If there are any spelling mistakes, remove the identified ones.\n"
      ],
      "metadata": {
        "id": "MmVP5TSas3Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing package to remove stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stpw=stopwords.words('english')\n",
        "print(stpw)\n",
        "stpw.extend(['tot'])\n",
        "print(stpw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d0a0ea-8a24-4b8d-cd51-795cffd12fd5",
        "id": "eDJ_YK5jvwjE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'tot']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. tot is considered as stop-words and removed from non stop word.\n",
        "stpwrd=[]\n",
        "nonstpwrd=[]\n",
        "for wrd in totw:\n",
        "  if wrd in stpw:\n",
        "    stpwrd.append(wrd)\n",
        "  else:\n",
        "    nonstpwrd.append(wrd)\n",
        "print(\"stopword list\", stpwrd)\n",
        "print(\"Non stop words list\", nonstpwrd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0167eb43-a0a8-41e1-b5eb-bd2226b88aca",
        "id": "GXchFfcqvwjE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopword list ['is', 'a', 'of', 'that', 'tot', 'with', 'and', 'the', 'tot', 'is', 'for', 'the', 'tot', 'be', 'tot', 'that', 'it', 'is', 'not', 'or', 'for', 'all', 'a', 'of', 'in', 'and', 'can', 'not', 'is', 'a', 'tot', 'that', 'a', 'of', 'and', 'the', 'tot', 'can', 'in', 'and', 'that', 'were', 'not', 'tot', 'it', 'at', 'the', 'of', 'its', 'with', 'a', 'and']\n",
            "Non stop words list ['Artificial', 'general', 'intelligence', '(', 'AGI', ')', 'field', 'theoretical', 'AI', 'research', 'attempts', 'create', 'software', 'human-like', 'intelligence', 'ability', 'self-teach', '.', 'The', 'aim', 'software', 'able', 'perform', 'tasks', 'necessarily', 'trained', 'developed', '.', 'Current', 'artificial', 'intelligence', '(', 'AI', ')', 'technologies', 'function', 'within', 'set', 'pre-determined', 'parameters', '.', 'For', 'example', ',', 'AI', 'models', 'trained', 'image', 'recognition', 'generation', 'build', 'websites', '.', 'AGI', 'theoretical', 'pursuit', 'develop', 'AI', 'systems', 'possess', 'autonomous', 'self-control', ',', 'reasonable', 'degree', 'self-understanding', ',', 'ability', 'learn', 'new', 'skills', '.', 'It', 'solve', 'complex', 'problems', 'settings', 'contexts', 'taught', 'time', 'creation', '.', 'AGI', 'human', 'abilities', 'remains', 'theoretical', 'concept', 'research', 'goal', '.']\n",
            "Length of stopwords: 52\n",
            "Length of non stop words 91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NHgWmEAyxt58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}